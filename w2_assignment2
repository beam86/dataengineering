/* week5 */
/* assignment2: Weather_Forecast DAG */

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
from  airflow.models import Variable
from airflow.hooks.postgres_hook import PostgresHook

import requests
import logging
import psycopg2
import json
from datetime import timedelta


WeatherDag = DAG (
    dag_id = 'Weather_Dag'
    , start_date = datetime(2021,12,04)
    , catchup=False
    , tags=['my_real_first_dag']
    , schedule_interval = "0/10 * * * *"
)

def get_Redshift_connection():
    hook = PostgresHook(postgres_conn_id='redshift_dev_db')
    return hook.get_conn().cursor()


def getWeatherData(**context) :

    latitude = context["params"]["latitude"]
    longitude = context["params"]["longitude"]
    app_key = context["params"]["app_key"]
    f = requests.get(f"https://api.openweathermap.org/data/2.5/onecall?lat={latitude}&lon={longitude}&exclude=current,minutely,hourly,alerts&appid={app_key}&units=metric")
    return f.json()

def transformWeatherData(**context) :

    ret = []
    weathers_js = context["task_instance"].xcom_pull(key="return_value", task_ids="getWeatherData")
    weatherObj = json.loads(json.dumps(weathers_js))
    weatherArr = weatherObj.get("daily")
    for weather in weatherArr :
        lst = []
        temp_dict = weather["temp"]
        lst.append(temp_dict["day"])
        lst.append(temp_dict["min"])
        lst.append(temp_dict["max"])
        lst.append(datetime.fromtimestamp(weather["dt"]).strftime('%Y-%m-%d'))
        ret.append(lst)

    return ret

def loadWeatherData(**context) :

    query_insert = ""
    cursor = get_Redshift_connection()
    weathers = context["task_instance"].xcom_pull(key="return_value", task_ids="transformWeatherData")
    for weather in weathers :
        date = weather[3]
        temp = weather[0]
        min_temp = weather[1]
        max_temp = weather[2]
        query_insert += f"insert into beam8686.temp_weather_information values('{date}', '{temp}', '{min_temp}', '{max_temp}');"
      
    try :
        cursor.execute("create table beam8686.temp_weather_information (date date, temp float, min_temp float, max_temp float, created_date timestamp default sysdate);")
	cursor.execute("insert into beam8686.temp_weather_information select * from beam8686.weather_information;")
        cursor.execute(query_insert)
        cursor.execute("delete from beam8686.weather_information;")
        cursor.execute("insert into beam8686.weather_information select date, temp, min_temp, max_temp, created_date from (select *, row_number() over (partition by date order by created-date) seq from beam8686.temp_weather_information) where seq = 1;")
        cursor.execute("drop table beam8686.temp_weather_information;")
        cursor.execute("commit;")
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
        cursor.execute("rollback;")

getWeatherData = PythonOperator (
    task_id = 'getWeatherData'
    , python_callable = getWeatherData
    , params = {
	  'url_format': Variable.get('url')
	  , 'latitude': Variable.get('latitude_seoul')
	  , 'longitude': Variable.get('longitude_seoul')
	  , 'app_key': Variable.get('open_weather_api_key')
    }
    , provide_context=True
    , dag = WeatherDag
)

transformWeatherData = PythonOperator (
    task_id = 'transformWeatherData'
    , python_callable = transformWeatherData
    , provide_context=True
    , dag = WeatherDag
)

loadWeatherData = PythonOperator (
    task_id = 'loadWeatherData'
    , python_callable = loadWeatherData
    , provide_context=True
    , dag = WeatherDag
)



#Assign the order of the tasks in our DAG
getWeatherData >> transformWeatherData >> loadWeatherData



/* assignment3: Quiz */

1. airflow.cfg
2. default_ui_timezone
3. auth_backend

5. 
sudo systemctl daemon-reload
sudo systemctl enable airflow-webserver
sudo systemctl enable airflow-scheduler

sudo systemctl start airflow-webserver
sudo systemctl start airflow-scheduler


-- for automatically setting --
airflow@ip-172-31-54-137:~$ vi ~/.bashrc 
AIRFLOW_HOME=/var/lib/airflow
export AIRFLOW_HOME
cd ~/




/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/









/* week4 */
/* assignment1 */

def ClearTable( cur ) :

  q_MakeTable = 'delete from beam8686.name_gender;'
  cur.execute(q_MakeTable)

def load(lines):
    # BEGIN과 END를 사용해서 SQL 결과를 트랜잭션으로 만들어주는 것이 좋음
    # BEGIN;DELETE FROM (본인의스키마).name_gender;INSERT INTO TABLE VALUES ('KEEYONG', 'MALE');....;END;
    cur = get_Redshift_connection()
    cur.execute('begin;')
    ClearTable(cur)
    for num in range(0, len(lines)):
        if lines[num] == '' or num == 0 :
            continue

        (name, gender) = lines[num].split(",")
        sql = "INSERT INTO beam8686.name_gender VALUES ('{n}', '{g}');".format(n=name, g=gender)
        cur.execute(sql)
        

    cur.execute('end;')

load(Lines)

#Lines에 split된 data가 저장되어 있다고 가정





/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/










/* week3*/
/* assignment 2 */

with users as (
  SELECT userid, channel
  , row_number() over(partition by userid order by ts ) as ranking
  , count(1) over ( partition by userid ) as cnt
FROM raw_data.user_session_channel usc
JOIN raw_data.session_timestamp st ON usc.sessionid = st.sessionid
where userid is not null
)

select a.userid as "user id", a.channel as "first channel", b.channel as "last channel"
from users as a
inner join users as b 
on b.userid = a.userid
and b.userid = 251
and b.ranking = b.cnt
where a.ranking = 1

/* assignment 3 */

select 
user_info.userid as userid
, sum(trns.amount) as grossRevenue
from raw_data.session_transaction as trns
left outer join raw_data.user_session_channel as user_info
on user_info.sessionid = trns.sessionid

group by user_info.userid
order by grossRevenue desc
limit 10;


/* assignment 4 */

create table beam8686.week3_assignment 
as select 
coalesce(to_char(dt.ts, 'yyyy-MM'), '9999-12') as year_month
, user_info.channel as channel
, count( distinct user_info.userid ) as uniqueUsers
, count( distinct case when trns.sessionid is not null and user_info.sessionid is not null then user_info.userid end ) as paidUsers
, concat(round((paidUsers::float / uniqueUsers::float)*100, 2)::text, '%') as conversionRate
, sum( trns.amount ) as grossRevenue
, sum( case when trns.refunded = false then trns.amount else 0 end ) as netRevenue
from raw_data.user_session_channel as user_info
left outer join raw_data.session_timestamp as dt on user_info.sessionid = dt.sessionid
left outer join raw_data.session_transaction as trns on user_info.sessionid = trns.sessionid   
where trns.amoutn > 0 
group by 1, 2
order by 1, 2;









/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/
/*---------------------------------------------------------------------*/

/*
mysql version
*/

select 
year(session_info.ts) as year
, month(session_info.ts) as month
, count(distinct user_info.user_id) as mau
from raw_data.user_session_channel as user_info
inner join raw_data.session_timestamp as session_info
on 1=1
and user_info.sessionid = session_info.sessionid 
group by year(session_info.ts), month(session_info.ts)



/*
postgresql version
*/
select 
extract(year from session_info.ts) as year
, extract(month from session_info.ts) as month
, count(distinct user_info.user_id) as mau
from raw_data.user_session_channel as user_info
inner join raw_data.session_timestamp as session_info
on 1=1
and user_info.sessionid = session_info.sessionid 
group by extract(year from session_info.ts), extract(month from session_info.ts)





